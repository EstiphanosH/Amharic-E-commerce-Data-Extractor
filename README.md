
# ğŸ›ï¸ Amharic E-commerce Data Extractor

A Telegram-based Amharic data ingestion, annotation, and model training pipeline for e-commerce and micro-lending insights. This project supports Named Entity Recognition (NER) in Amharic and vendor analytics using scraped messages from Ethiopian-based Telegram vendors.

---

## ğŸ“š Table of Contents

- [ğŸ›ï¸ Amharic E-commerce Data Extractor](#ï¸-amharic-e-commerce-data-extractor)
  - [ğŸ“š Table of Contents](#-table-of-contents)
  - [ğŸ“Œ Project Overview](#-project-overview)
  - [ğŸ’» Technology Stack](#-technology-stack)
  - [âš™ï¸ System Requirements](#ï¸-system-requirements)
  - [ğŸ“ Project Structure](#-project-structure)
  - [ğŸ§  Task Summaries](#-task-summaries)
    - [âœ… Task 1: Data Collection \& Preprocessing](#-task-1-data-collection--preprocessing)
    - [âœ… Task 2: Labeling in CoNLL Format](#-task-2-labeling-in-conll-format)
    - [âœ… Task 3: Fine-Tune NER Model](#-task-3-fine-tune-ner-model)
    - [âœ… Task 4: Model Comparison](#-task-4-model-comparison)
    - [âœ… Task 5: Model Interpretability](#-task-5-model-interpretability)
    - [âœ… Task 6: Vendor Scorecard for Lending](#-task-6-vendor-scorecard-for-lending)
  - [ğŸ”– Annotation Schema](#-annotation-schema)
  - [ğŸ“Š Demo Notebooks](#-demo-notebooks)
  - [ğŸš€ Getting Started](#-getting-started)
  - [ğŸ“Œ License](#-license)
  - [This project is intended for academic and research use.](#this-project-is-intended-for-academic-and-research-use)
  - [ğŸ™Œ Acknowledgements](#-acknowledgements)

---

## ğŸ“Œ Project Overview

This project was built to:
- Ingest and preprocess Amharic-language Telegram e-commerce data
- Annotate and fine-tune models for Named Entity Recognition (NER)
- Evaluate and compare multilingual models for Amharic NER
- Interpret and explain NER model predictions
- Build a FinTech-ready vendor analytics and lending scorecard

---

## ğŸ’» Technology Stack

- **Python 3.10+**
- **Telethon** â€“ Telegram data scraping
- **HuggingFace Transformers** â€“ Fine-tuning multilingual NER models
- **spaCy, Pandas, Regex** â€“ Preprocessing and evaluation
- **SHAP & LIME** â€“ Model interpretability
- **Google Colab / Jupyter Notebooks** â€“ Development environment
- **GitHub Actions** â€“ CI/CD and version control

---

## âš™ï¸ System Requirements

- Python 3.10+
- pip / conda
- Telegram API credentials
- Access to GPU (Colab Pro, local CUDA, or AWS)

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ .github/workflows/            # CI/CD workflows
â”œâ”€â”€ .vscode/                      # VSCode workspace config
â”œâ”€â”€ app/                          # Web app backend (future use)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml               # Global parameters and channel lists
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ NER_Labeling_Task.ipynb       # Manual labeling in CoNLL
â”‚   â”œâ”€â”€ preprocessing.ipynb           # Cleaning and tokenizing messages
â”‚   â”œâ”€â”€ telegram_scraping.ipynb       # Data scraping pipeline
â”‚   â”œâ”€â”€ finetune_NER_model.ipynb      # Model training
â”‚   â”œâ”€â”€ model_comparison.ipynb        # Side-by-side model evaluation
â”‚   â”œâ”€â”€ model_interpretability.ipynb  # SHAP/LIME explanations
â”‚   â””â”€â”€ vendor_scorecard.ipynb        # FinTech analytics + lending score
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ telegram_scraper.py
â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”œâ”€â”€ ner_trainer.py
â”‚   â”œâ”€â”€ ner_data_utils.py
â”‚   â”œâ”€â”€ model_interpret.py
â”‚   â”œâ”€â”€ coll_annotator.py
â”‚   â””â”€â”€ vendor_analytics.py
â”œâ”€â”€ src/                       # Internal packages
â”œâ”€â”€ tests/                     # Unit and functional tests
â”œâ”€â”€ utils/                     # Utility modules
â”œâ”€â”€ main.py
â””â”€â”€ README.md
```

---

## ğŸ§  Task Summaries

### âœ… Task 1: Data Collection & Preprocessing
- Scraped Amharic messages, images, and metadata using `telethon`.
- Normalized text, tokenized Amharic, separated metadata.
- Output structured dataset saved for annotation.

### âœ… Task 2: Labeling in CoNLL Format
- Manually annotated 50+ messages with entity tags: `B/I-Product`, `B/I-LOC`, `B/I-PRICE`, `O`.
- Saved labels in standard CoNLL format for training.

### âœ… Task 3: Fine-Tune NER Model
- Trained XLM-Roberta, AfriBERTa, and mBERT using Hugging Face.
- Used `ner_trainer.py` to handle training loop, tokenizer, and CoNLL loader.
- Tuned learning rate, epochs, and batch sizes in `finetune_NER_model.ipynb`.

### âœ… Task 4: Model Comparison
- Benchmarked models by F1 score and training speed.
- Used `model_comparison.ipynb` to visualize performance.
- Selected XLM-Roberta for highest accuracy.

### âœ… Task 5: Model Interpretability
- Used SHAP and LIME to explain why models tagged tokens as entities.
- Difficult cases were manually reviewed in `model_interpretability.ipynb`.

### âœ… Task 6: Vendor Scorecard for Lending
- Created `vendor_analytics.py` to calculate:
  - Avg. Posts/Week
  - Avg. Views/Post
  - Avg. Price
  - Top Performing Product
- Final vendor score:  
  `Score = (Avg Views Ã— 0.5) + (Post Freq Ã— 0.5)`

---

## ğŸ”– Annotation Schema

| Tag        | Description                      |
|------------|----------------------------------|
| B-Product  | Start of product name            |
| I-Product  | Continuation of product name     |
| B-PRICE    | Start of price entity            |
| I-PRICE    | Continuation of price entity     |
| B-LOC      | Start of location                |
| I-LOC      | Continuation of location         |
| O          | Outside any named entity         |

---

## ğŸ“Š Demo Notebooks

- `notebooks/NER_Labeling_Task.ipynb`
- `notebooks/telegram_scraping.ipynb`
- `notebooks/finetune_NER_model.ipynb`
- `notebooks/model_comparison.ipynb`
- `notebooks/vendor_scorecard.ipynb`

---

## ğŸš€ Getting Started

```bash
git clone https://github.com/EstiphanosH/Amharic-Ecommerce-Data-Extractor.git
cd Amharic-Ecommerce-Data-Extractor
pip install -r requirements.txt

# Example: Run scraper
python scripts/telegram_scraper.py

# Preprocess messages
python scripts/preprocessor.py
```

---

## ğŸ“Œ License

This project is intended for academic and research use. 
---

## ğŸ™Œ Acknowledgements

- Built as part of 10 Academyâ€™s NLP track
- Special thanks to open-source communities supporting low-resource languages
